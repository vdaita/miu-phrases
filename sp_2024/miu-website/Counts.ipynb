{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63d2182-3724-4321-b567-9c93bec02bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5224b8-52a1-4fad-a7e3-51a772da1759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197/3174632621.py:2: DtypeWarning: Columns (300,304,325,328) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"company_website_second_round_with_additional_firms.csv\", low_memory=True, index_col=[0])\n"
     ]
    }
   ],
   "source": [
    "# Changed to index_col=[0] to prevent Unnamed: X columns\n",
    "df = pd.read_csv(\"company_website_second_round_with_additional_firms.csv\", low_memory=True, index_col=[0])\n",
    "df = df.drop(columns=[col for col in df.columns if col.startswith('Unnamed:')]) # If th\n",
    "\n",
    "non_temporal_df = df.iloc[:, 0:13] # Will need to change indexing if more non-temporal columns are added\n",
    "df = df.drop(df.columns[0:13], axis=1)\n",
    "assert not any(re.match(r\"\\d{4}-\\d{2}\", col) for col in non_temporal_df.columns), \"Columns with the pattern 'YYYY-MM' are present in the non-temporal dataframe\"\n",
    "assert all(re.match(r\"\\d{4}-\\d{2}\", col) for col in df.columns), \"Not all columns follow the 'YYYY-MM' pattern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfee24b-b358-4630-a4e6-bf1272ce7899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5189, 325)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b09bdae-c44d-4176-80de-3b1ea7237897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hash(string):\n",
    "    ans = 0\n",
    "    for i in range(0, min(len(string), 8)):\n",
    "        ans += ord(string[i]) * (7**i)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aca4b7e-8d82-4a9b-b2cc-a42fde6e7601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 0 out of 5189 rows.\n",
      "Finished processing 100 out of 5189 rows.\n",
      "Finished processing 200 out of 5189 rows.\n",
      "Finished processing 300 out of 5189 rows.\n",
      "Finished processing 400 out of 5189 rows.\n",
      "Finished processing 500 out of 5189 rows.\n",
      "Finished processing 600 out of 5189 rows.\n",
      "Finished processing 700 out of 5189 rows.\n",
      "Finished processing 800 out of 5189 rows.\n",
      "Finished processing 900 out of 5189 rows.\n",
      "Finished processing 1000 out of 5189 rows.\n",
      "Finished processing 1100 out of 5189 rows.\n",
      "Finished processing 1200 out of 5189 rows.\n",
      "Finished processing 1300 out of 5189 rows.\n",
      "Finished processing 1400 out of 5189 rows.\n",
      "Finished processing 1500 out of 5189 rows.\n",
      "Finished processing 1600 out of 5189 rows.\n",
      "Finished processing 1700 out of 5189 rows.\n",
      "Finished processing 1800 out of 5189 rows.\n",
      "Finished processing 1900 out of 5189 rows.\n",
      "Finished processing 2000 out of 5189 rows.\n",
      "Finished processing 2100 out of 5189 rows.\n",
      "Finished processing 2200 out of 5189 rows.\n",
      "Finished processing 2300 out of 5189 rows.\n",
      "Finished processing 2400 out of 5189 rows.\n",
      "Finished processing 2500 out of 5189 rows.\n",
      "Finished processing 2600 out of 5189 rows.\n",
      "Finished processing 2700 out of 5189 rows.\n",
      "Finished processing 2800 out of 5189 rows.\n",
      "Finished processing 2900 out of 5189 rows.\n",
      "Finished processing 3000 out of 5189 rows.\n",
      "Finished processing 3100 out of 5189 rows.\n",
      "Finished processing 3200 out of 5189 rows.\n",
      "Finished processing 3300 out of 5189 rows.\n",
      "Finished processing 3400 out of 5189 rows.\n",
      "Finished processing 3500 out of 5189 rows.\n",
      "Finished processing 3600 out of 5189 rows.\n",
      "Finished processing 3700 out of 5189 rows.\n",
      "Finished processing 3800 out of 5189 rows.\n",
      "Finished processing 3900 out of 5189 rows.\n",
      "Finished processing 4000 out of 5189 rows.\n",
      "Finished processing 4100 out of 5189 rows.\n",
      "Finished processing 4200 out of 5189 rows.\n",
      "Finished processing 4300 out of 5189 rows.\n",
      "Finished processing 4400 out of 5189 rows.\n",
      "Finished processing 4500 out of 5189 rows.\n",
      "Finished processing 4600 out of 5189 rows.\n",
      "Finished processing 4700 out of 5189 rows.\n",
      "Finished processing 4800 out of 5189 rows.\n",
      "Finished processing 4900 out of 5189 rows.\n",
      "Finished processing 5000 out of 5189 rows.\n",
      "Finished processing 5100 out of 5189 rows.\n",
      "478736 14980\n"
     ]
    }
   ],
   "source": [
    "table = {}\n",
    "saved = 0\n",
    "considering = 0\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Finished processing {index} out of {len(df.index)} rows.\")\n",
    "    # print(len(row))\n",
    "    for column_index, value in enumerate(row):\n",
    "        if pd.isna(value) or type(value) == int or type(value) == float:\n",
    "            continue\n",
    "        value_hash = compute_hash(value)\n",
    "        \n",
    "        if not(value_hash in table):\n",
    "            table[value_hash] = []\n",
    "\n",
    "        max_similarity = 0\n",
    "        for existing_value in table[value_hash]:\n",
    "            max_similarity = max(max_similarity, fuzz.ratio(existing_value, value))\n",
    "\n",
    "        if max_similarity < 0.95:\n",
    "            table[value_hash].append(value)\n",
    "            considering += 1\n",
    "        else:\n",
    "            saved += 1\n",
    "            # print(index, column_index)\n",
    "            df.iloc[index, column_index - 1] = -1 # subtracting 1 from the column_index because the first one is probably something irrelevant\n",
    "            \n",
    "print(saved, considering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4b8b77e-604a-4986-8791-595fa2f9436f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 0 out of 5189 rows.\n",
      "Finished processing 100 out of 5189 rows.\n",
      "Finished processing 200 out of 5189 rows.\n",
      "Finished processing 300 out of 5189 rows.\n",
      "Finished processing 400 out of 5189 rows.\n",
      "Finished processing 500 out of 5189 rows.\n",
      "Finished processing 600 out of 5189 rows.\n",
      "Finished processing 700 out of 5189 rows.\n",
      "Finished processing 800 out of 5189 rows.\n",
      "Finished processing 900 out of 5189 rows.\n",
      "Finished processing 1000 out of 5189 rows.\n",
      "Finished processing 1100 out of 5189 rows.\n",
      "Finished processing 1200 out of 5189 rows.\n",
      "Finished processing 1300 out of 5189 rows.\n",
      "Finished processing 1400 out of 5189 rows.\n",
      "Finished processing 1500 out of 5189 rows.\n",
      "Finished processing 1600 out of 5189 rows.\n",
      "Finished processing 1700 out of 5189 rows.\n",
      "Finished processing 1800 out of 5189 rows.\n",
      "Finished processing 1900 out of 5189 rows.\n",
      "Finished processing 2000 out of 5189 rows.\n",
      "Finished processing 2100 out of 5189 rows.\n",
      "Finished processing 2200 out of 5189 rows.\n",
      "Finished processing 2300 out of 5189 rows.\n",
      "Finished processing 2400 out of 5189 rows.\n",
      "Finished processing 2500 out of 5189 rows.\n",
      "Finished processing 2600 out of 5189 rows.\n",
      "Finished processing 2700 out of 5189 rows.\n",
      "Finished processing 2800 out of 5189 rows.\n",
      "Finished processing 2900 out of 5189 rows.\n",
      "Finished processing 3000 out of 5189 rows.\n",
      "Finished processing 3100 out of 5189 rows.\n",
      "Finished processing 3200 out of 5189 rows.\n",
      "Finished processing 3300 out of 5189 rows.\n",
      "Finished processing 3400 out of 5189 rows.\n",
      "Finished processing 3500 out of 5189 rows.\n",
      "Finished processing 3600 out of 5189 rows.\n",
      "Finished processing 3700 out of 5189 rows.\n",
      "Finished processing 3800 out of 5189 rows.\n",
      "Finished processing 3900 out of 5189 rows.\n",
      "Finished processing 4000 out of 5189 rows.\n",
      "Finished processing 4100 out of 5189 rows.\n",
      "Finished processing 4200 out of 5189 rows.\n",
      "Finished processing 4300 out of 5189 rows.\n",
      "Finished processing 4400 out of 5189 rows.\n",
      "Finished processing 4500 out of 5189 rows.\n",
      "Finished processing 4600 out of 5189 rows.\n",
      "Finished processing 4700 out of 5189 rows.\n",
      "Finished processing 4800 out of 5189 rows.\n",
      "Finished processing 4900 out of 5189 rows.\n",
      "Finished processing 5000 out of 5189 rows.\n",
      "Finished processing 5100 out of 5189 rows.\n",
      "14980\n"
     ]
    }
   ],
   "source": [
    "actual_text_count = 0\n",
    "\n",
    "for index, row in enumerate(df.itertuples()):\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Finished processing {index} out of {len(df.index)} rows.\")\n",
    "    # print(len(row))\n",
    "    for column_index, value in enumerate(row):\n",
    "        if pd.isna(value) or type(value) == int or type(value) == float:\n",
    "            continue\n",
    "        actual_text_count += 1\n",
    "\n",
    "print(actual_text_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "add590e2-7c60-4b4f-a468-1b0f11026e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(corpus, length=20, split=15):\n",
    "    total_words = [word.lower() for word in corpus.split()]\n",
    "    phrases = []\n",
    "    \n",
    "    for i in range(0, len(total_words), split):\n",
    "        right_idx = min(len(total_words), i + length)\n",
    "        phrases.append(\" \".join(total_words[i:right_idx]))\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b66731b-dc9e-43ec-bee0-4a17404eadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "embeddings_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "032fba2b-e41a-4670-96af-b13fa5e74fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_files = [\n",
    "    {\"keyword\": \"antiforeign\", \"file\": \"antiforeign_revised_2_phrases.json\"},\n",
    "    {\"keyword\": \"military\", \"file\": \"military_revised_phrases.json\"},\n",
    "    {\"keyword\": \"national_pride\", \"file\": \"national_pride_revised_phrases.json\"},\n",
    "    {\"keyword\": \"quality\", \"file\": \"quality_revised_phrases.json\"},\n",
    "    {\"keyword\": \"revival\", \"file\": \"revival_phrases.json\"},\n",
    "    {\"keyword\": \"jobs\", \"file\": \"jobs_revised_phrases.json\"},\n",
    "    {\"keyword\": \"labor\", \"file\": \"labor_revised_phrases.json\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137690a-70b9-4f3d-9f1c-013c21a17359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current keyword being processed:  antiforeign\n",
      "Processed 500 cells in 13.90708589553833\n",
      "Processed 1000 cells in 15.090537071228027\n",
      "Processed 1500 cells in 13.737221956253052\n",
      "Processed 2000 cells in 13.054450750350952\n",
      "Processed 2500 cells in 14.681780338287354\n",
      "Processed 3000 cells in 12.620123386383057\n",
      "Processed 3500 cells in 12.826378583908081\n",
      "Processed 4000 cells in 13.61329460144043\n",
      "Processed 4500 cells in 15.889739274978638\n",
      "Processed 5000 cells in 12.73293399810791\n",
      "Processed 5500 cells in 11.786227941513062\n",
      "Processed 6000 cells in 11.5247323513031\n",
      "Processed 6500 cells in 9.94400954246521\n",
      "Processed 7000 cells in 9.454385995864868\n",
      "Processed 7500 cells in 8.101589679718018\n",
      "Processed 8000 cells in 9.298517227172852\n",
      "Processed 8500 cells in 8.685803890228271\n",
      "Processed 9000 cells in 8.012553453445435\n",
      "Processed 9500 cells in 6.9875335693359375\n",
      "Processed 10000 cells in 7.3401923179626465\n",
      "Processed 10500 cells in 7.500226736068726\n",
      "Processed 11000 cells in 6.128046035766602\n",
      "Processed 11500 cells in 5.5723350048065186\n",
      "Processed 12000 cells in 5.617940425872803\n",
      "Processed 12500 cells in 5.139404296875\n",
      "Processed 13000 cells in 4.505764007568359\n",
      "Processed 13500 cells in 4.446840763092041\n",
      "Processed 14000 cells in 4.186155796051025\n",
      "Processed 14500 cells in 3.96173357963562\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "for keyword_file in keyword_files:\n",
    "    keyword = keyword_file[\"keyword\"]\n",
    "    filename = keyword_file[\"file\"]\n",
    "\n",
    "    existing_phrases_file = open(\"antiforeign_revised_2_phrases.json\", \"r\")\n",
    "    existing_phrases = json.loads(existing_phrases_file.read())[:10]\n",
    "    existing_embedded = embeddings_model.encode(existing_phrases, convert_to_tensor=True)\n",
    "\n",
    "    counter = 0\n",
    "    time_last = time.time()\n",
    "    similar_phrases = []\n",
    "    \n",
    "    non_llm_map = {}\n",
    "    keyword = \"antiforeign\"\n",
    "    total_phrase_count = 0\n",
    "    \n",
    "    def count_phrases(text):\n",
    "        global time_last\n",
    "        global counter \n",
    "        global similar_phrases\n",
    "        global base_embedded\n",
    "        global total_phrase_count\n",
    "        \n",
    "        if pd.isna(text) or type(text) == float or type(text) == int:\n",
    "            return text\n",
    "            \n",
    "        phrases = get_phrases(text)\n",
    "        if len(phrases) == 0:\n",
    "            phrases = [text]\n",
    "    \n",
    "        phrase_count = 0\n",
    "        phrases_embedded = embeddings_model.encode(phrases, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine-similarities for each sentence with each other sentence\n",
    "        cosine_scores = util.cos_sim(existing_embedded, phrases_embedded)\n",
    "        \n",
    "        # Find the pairs with the highest cosine similarity scores\n",
    "        for j in range(cosine_scores.shape[1]): # Iterating through all of the corpus phrases\n",
    "            for i in range(cosine_scores.shape[0]): # Iterating through all of the existing original phrases\n",
    "                if cosine_scores[i][j] > 0.4:\n",
    "                    phrase_count += 1\n",
    "                    total_phrase_count += 1\n",
    "                    break # Already found an existing original phrase that is similar enough - break\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "        if counter % 500 == 0:\n",
    "            print(f\"Processed {counter} cells in {time.time() - time_last}\")\n",
    "            time_last = time.time()\n",
    "        \n",
    "        return phrase_count\n",
    "\n",
    "    all_similar_phrases = {}\n",
    "    print(\"Current keyword being processed: \", keyword)\n",
    "    df2 = df.map(count_phrases)\n",
    "    all_similar_phrases[keyword] = similar_phrases\n",
    "    df2.to_csv(f\"count_csvs_4/{keyword}_simsearch_nofix_2.csv\")\n",
    "    df2 = df2.drop(columns=[col for col in df2.columns if col.startswith('Unnamed:')]) # If th\n",
    "    for index, row in df2.iterrows():\n",
    "        prev_value = None\n",
    "        for col in reversed(df2.columns):\n",
    "            value = row[col]\n",
    "            if pd.isna(value) or value == -1 or value == 0:\n",
    "                if prev_value is not None:\n",
    "                    df2.at[index, col] = prev_value\n",
    "            elif value == -1:\n",
    "                df2.at[index, col] = 0\n",
    "            else:\n",
    "                prev_value = value\n",
    "    print(df2.sum())\n",
    "    df2.to_csv(f\"count_csvs_4/prelim_{keyword}_simsearch4.csv\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
