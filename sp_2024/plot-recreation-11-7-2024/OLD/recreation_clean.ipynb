{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"made in america\", \"made in u.s.\", \"made in us\",\n",
    "    \"american made\", \"usa made\", \"u.s. made\", \"us made\",\n",
    "    \"buy american\", \"buy usa\", \"buy america\",\n",
    "    \"support america\", \"support usa\", \"support u.s.\",\n",
    "    \"patriot\", \"choose american\", \"choose usa\", \"choose u.s.\", \"choose america\",\n",
    "    \"national pride\", \"usa based\", \"america based\", \"american based\", \"us based\", \"u.s. based\",\n",
    "    \"usa produced\", \"america produced\", \"american produced\", \"us produced\", \"u.s. produced\",\n",
    "    \"usa manufactured\", \"america manufactured\", \"american manufactured\", \"us manufactured\", \"u.s. manufactured\",\n",
    "    \"american worker\", \"american job\", \"veteran owned\", \"veteran founded\", \"founded by veteran\",\n",
    "    \"handcrafted in america\", \"handcrafted in usa\", \"handcrafted in u.s.\", \"handcrafted in us\",\n",
    "    \"crafted in america\", \"crafted in u.s.\", \"crafted in us\",\n",
    "    \"america heritage\", \"america tradition\", \"america value\",\n",
    "    \"icon of america\", \"icon of usa\", \"icon of u.s.\",\n",
    "    \"america manufactur\", \"u.s. manufactur\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "def load_and_process_file(file_path):\n",
    "    def filter_columns(df):\n",
    "        pattern = r\"^\\d{4}-\\d{2}$\"\n",
    "        filtered_columns = ['std_name'] + [col for col in df.columns if re.match(pattern, col)]\n",
    "        return df[filtered_columns]\n",
    "\n",
    "    # Load only the first 100,000 rows for testing\n",
    "    df = pd.read_csv(file_path, nrows=100000, low_memory=False)\n",
    "    df = filter_columns(df)\n",
    "    df = df.drop_duplicates(subset='std_name').sort_values(by='std_name').reset_index(drop=True)\n",
    "    df = df.drop(columns=['std_name'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_process_file(\"buy_usa_second_round_with_additional_firms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2023-10</th>\n",
       "      <th>2023-09</th>\n",
       "      <th>2023-08</th>\n",
       "      <th>2023-07</th>\n",
       "      <th>2023-06</th>\n",
       "      <th>2023-05</th>\n",
       "      <th>2023-04</th>\n",
       "      <th>2023-03</th>\n",
       "      <th>2023-02</th>\n",
       "      <th>2023-01</th>\n",
       "      <th>...</th>\n",
       "      <th>1997-07</th>\n",
       "      <th>1997-06</th>\n",
       "      <th>1997-05</th>\n",
       "      <th>1997-04</th>\n",
       "      <th>1997-03</th>\n",
       "      <th>1997-02</th>\n",
       "      <th>1997-01</th>\n",
       "      <th>1996-12</th>\n",
       "      <th>1996-11</th>\n",
       "      <th>1996-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14th Rose - Handbags, Purses, Woman's Bags\\n F...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14th Rose - Handbags, Purses, Woman's Bags\\n F...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14th Rose - Handbags, Purses, Woman's Bags\\n F...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>About Soy. 1803 Candles - Best Scented Soy Can...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>About Soy. 1803 Candles - Best Scented Soy Can...</td>\n",
       "      <td>About Soy. 1803 Candles - Best Scented Soy Can...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>About Soy. 1803 Candles - Best Scented Soy Can...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             2023-10 2023-09 2023-08 2023-07  \\\n",
       "0                                                NaN     NaN     NaN     NaN   \n",
       "1                                                NaN     NaN     NaN     NaN   \n",
       "2                                                NaN     NaN     NaN     NaN   \n",
       "3                                                NaN     NaN     NaN     NaN   \n",
       "4  About Soy. 1803 Candles - Best Scented Soy Can...     NaN     NaN     NaN   \n",
       "\n",
       "                                             2023-06  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  14th Rose - Handbags, Purses, Woman's Bags\\n F...   \n",
       "3                                                NaN   \n",
       "4  About Soy. 1803 Candles - Best Scented Soy Can...   \n",
       "\n",
       "                                             2023-05 2023-04  \\\n",
       "0                                                NaN     NaN   \n",
       "1                                                NaN     NaN   \n",
       "2                                                NaN     NaN   \n",
       "3                                                NaN     NaN   \n",
       "4  About Soy. 1803 Candles - Best Scented Soy Can...     NaN   \n",
       "\n",
       "                                             2023-03 2023-02  \\\n",
       "0                                                NaN     NaN   \n",
       "1                                                NaN     NaN   \n",
       "2  14th Rose - Handbags, Purses, Woman's Bags\\n F...     NaN   \n",
       "3                                                NaN     NaN   \n",
       "4  About Soy. 1803 Candles - Best Scented Soy Can...     NaN   \n",
       "\n",
       "                                             2023-01  ... 1997-07 1997-06  \\\n",
       "0                                                NaN  ...     NaN     NaN   \n",
       "1                                                NaN  ...     NaN     NaN   \n",
       "2  14th Rose - Handbags, Purses, Woman's Bags\\n F...  ...     NaN     NaN   \n",
       "3                                                NaN  ...     NaN     NaN   \n",
       "4                                                NaN  ...     NaN     NaN   \n",
       "\n",
       "  1997-05 1997-04 1997-03 1997-02 1997-01 1996-12 1996-11 1996-10  \n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data in chunks\n",
    "def load_and_process_file(file_path):\n",
    "    def filter_columns(df):\n",
    "        pattern = r\"^\\d{4}-\\d{2}$\"\n",
    "        filtered_columns = ['std_name'] + [col for col in df.columns if re.match(pattern, col)]\n",
    "        return df[filtered_columns]\n",
    "\n",
    "    df_chunk = pd.read_csv(file_path, chunksize=5000)\n",
    "    results = []\n",
    "    for chunk in df_chunk:\n",
    "        chunk = filter_columns(chunk)\n",
    "        chunk = chunk.drop_duplicates(subset='std_name').sort_values(by='std_name').reset_index(drop=True)\n",
    "        chunk = chunk.drop(columns=['std_name'])\n",
    "        results.append(chunk)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Keyword counting functions\n",
    "def count_keywords_in_cell(cell, keywords):\n",
    "    return sum(1 if keyword.lower() in str(cell).lower() else 0 for keyword in keywords)\n",
    "\n",
    "def calculate_total_counts(df, columns, keywords):\n",
    "    import pandas as pd  # Local import for multiprocessing context\n",
    "    total_counts_df = pd.DataFrame(index=df.index, columns=columns, dtype=int).fillna(0)\n",
    "    for index, row in df.iterrows():\n",
    "        previous_count = 0\n",
    "        for col_idx, col in enumerate(reversed(columns)):\n",
    "            cell_value = row[col]\n",
    "            current_count = count_keywords_in_cell(cell_value, keywords)\n",
    "            if current_count == 0 and previous_count > 0:\n",
    "                total_counts_df.at[index, columns[len(columns) - 1 - col_idx]] = previous_count\n",
    "            else:\n",
    "                total_counts_df.at[index, columns[len(columns) - 1 - col_idx]] = current_count\n",
    "                previous_count = current_count\n",
    "    return total_counts_df\n",
    "\n",
    "# Parallel keyword existence calculation\n",
    "def count_term_existence(df, columns, total_counts_df, keyword):\n",
    "    import pandas as pd  # Local import for multiprocessing context\n",
    "    term_existence = [0] * len(columns)\n",
    "    for index, row in df.iterrows():\n",
    "        previous_total_count = 0\n",
    "        for col_idx, col in enumerate(reversed(columns)):\n",
    "            cell_value = row[col]\n",
    "            current_total_count = total_counts_df.at[index, col]\n",
    "            if current_total_count == 0 and previous_total_count > 0:\n",
    "                term_existence[len(columns) - 1 - col_idx] += 1\n",
    "            elif pd.isna(cell_value) or isinstance(cell_value, int):\n",
    "                term_existence[len(columns) - 1 - col_idx] += 0\n",
    "            else:\n",
    "                term_existence[len(columns) - 1 - col_idx] += 1 if keyword in str(cell_value).lower() else 0\n",
    "            previous_total_count = max(current_total_count, previous_total_count)\n",
    "    return {keyword: sum(term_existence)}\n",
    "\n",
    "# Generate final value for each year with TF-IDF\n",
    "def generate_final_value_by_year(df, total_counts_df, columns, document_count_sum, data):\n",
    "    import pandas as pd  # Local import for multiprocessing context\n",
    "    import math\n",
    "    keyword, term_existence = data\n",
    "    term_existence_full = [0] * len(columns)\n",
    "    if term_existence == 0:\n",
    "        return {keyword: term_existence_full}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        previous_value = 0\n",
    "        for col_idx, col in enumerate(reversed(columns)):\n",
    "            cell_value = row[col]\n",
    "            current_total_count = total_counts_df.at[index, col]\n",
    "            idf = math.log(1 + (document_count_sum / term_existence))\n",
    "            keyword_count = str(cell_value).lower().count(keyword)\n",
    "            value = (keyword_count * idf / len(str(cell_value))) * 100\n",
    "            term_existence_full[len(columns) - 1 - col_idx] += value\n",
    "            previous_value = value\n",
    "    return {keyword: term_existence_full}\n",
    "\n",
    "# Multiprocessing setup\n",
    "def run_keyword_analysis(file_paths):\n",
    "    all_tf_idf_totals = {}\n",
    "    for file_path in file_paths:\n",
    "        df = load_and_process_file(file_path)\n",
    "        columns = list(df.columns)\n",
    "        total_counts_df = calculate_total_counts(df, columns, keywords)\n",
    "        document_count_sum = total_counts_df.sum().sum()\n",
    "        \n",
    "        func = partial(count_term_existence, df, columns, total_counts_df)\n",
    "        with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "            term_counts = list(executor.map(func, keywords))\n",
    "\n",
    "        term_count_dict = {k: v for d in term_counts for k, v in d.items()}\n",
    "        func = partial(generate_final_value_by_year, df, total_counts_df, columns, document_count_sum)\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=6) as executor:\n",
    "            tf_idf_results = list(executor.map(func, ((keyword, term_count_dict[keyword]) for keyword in keywords)))\n",
    "\n",
    "        tf_idf_total = {k: v for d in tf_idf_results for k, v in d.items()}\n",
    "        all_tf_idf_totals[file_path] = tf_idf_total\n",
    "    \n",
    "    return all_tf_idf_totals\n",
    "\n",
    "# Plotting function for all datasets on the same graph\n",
    "def plot_results(all_tf_idf_totals, columns_trimmed):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label, tf_idf_total in all_tf_idf_totals.items():\n",
    "        combined_values = [sum(values) for values in zip(*tf_idf_total.values())]\n",
    "        plt.plot(columns_trimmed, combined_values, label=label)\n",
    "    plt.legend()\n",
    "    plt.title('TF-IDF Values Over Time for Different Data Sources')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('TF-IDF Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"buy_usa_second_round_with_additional_firms.csv\",\n",
    "    \"about_us_second_round_with_additional_firms.csv\",\n",
    "    \"company_website_second_round_with_additional_firms.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theal\\AppData\\Local\\Temp\\ipykernel_13108\\2010235640.py:10: DtypeWarning: Columns (189,229,268,276,278,284,286,297,302,313,315,323,324,326,330,339) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in df_chunk:\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_tf_idf_totals \u001b[38;5;241m=\u001b[39m \u001b[43mrun_keyword_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m columns_trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_tf_idf_totals[file_paths[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Use columns from the first dataset\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plot_results(all_tf_idf_totals, columns_trimmed)\n",
      "Cell \u001b[1;32mIn[19], line 87\u001b[0m, in \u001b[0;36mrun_keyword_analysis\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m     85\u001b[0m func \u001b[38;5;241m=\u001b[39m partial(count_term_existence, df, columns, total_counts_df)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 87\u001b[0m     term_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m term_count_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m term_counts \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m d\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     90\u001b[0m func \u001b[38;5;241m=\u001b[39m partial(generate_final_value_by_year, df, total_counts_df, columns, document_count_sum)\n",
      "File \u001b[1;32mc:\\Users\\theal\\.conda\\envs\\linegym\\Lib\\concurrent\\futures\\process.py:642\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theal\\.conda\\envs\\linegym\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\theal\\.conda\\envs\\linegym\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\theal\\.conda\\envs\\linegym\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\theal\\.conda\\envs\\linegym\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "all_tf_idf_totals = run_keyword_analysis(file_paths)\n",
    "columns_trimmed = list(all_tf_idf_totals[file_paths[0]].values())[0]  # Use columns from the first dataset\n",
    "plot_results(all_tf_idf_totals, columns_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_trimmed = list(all_tf_idf_totals[file_paths[0]].values())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(all_tf_idf_totals, columns_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linegym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
