{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"bigram_model.model\")\n",
    "model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332136"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words_antiforeign = [\n",
    "    \"foreign\", \"outsider\", \"alien\", \"overseas\", \"imported\", \"external\", \"foreign-made\",\n",
    "    \"unpatriotic\", \"un-American\", \"unauthorized\", \"counterfeit\", \"offshore\"\n",
    "]\n",
    "\n",
    "seed_words_fairness = [\n",
    "    \"labor\", \"unions\", \"rights\", \"equality\", \"justice\", \"diversity\", \"inclusion\", \n",
    "    \"fairness\", \"safety\", \"wages\", \"benefits\", \"respect\", \"dignity\", \n",
    "    \"opportunity\", \"representation\"\n",
    "]\n",
    "\n",
    "seed_words_job_growth = [\n",
    "    \"local\", \"hire american\", \"growth\", \"opportunity\", \"employment\", \"workforce\", \n",
    "    \"development\", \"innovation\", \"careers\", \"training\", \"industries\", \"jobs\", \n",
    "    \"economy\", \"expansion\", \"businesses\", \"investment\", \"prosperity\", \"entrepreneurship\", \n",
    "    \"skilled labor\"\n",
    "]\n",
    "\n",
    "seed_words_military = [\n",
    "    \"service\", \"honor\", \"valor\", \"duty\", \"patriotism\", \n",
    "    \"sacrifice\", \"courage\", \"mission\", \"integrity\", \"loyalty\", \n",
    "    \"freedom\", \"strength\", \"security\", \"leadership\", \"heroes\"\n",
    "]\n",
    "\n",
    "seed_words_miu = [\n",
    "    \"domestic\", \"local\", \"patriotic\", \"homegrown\", \"American-made\",\n",
    "    \"regional\", \"community\", \"in-house\",\n",
    "    \"national\", \"loyal\", \"heritage\"\n",
    "]\n",
    "\n",
    "seed_words_pride =  [\n",
    "    \"american\",\n",
    "    \"usa-made\",\n",
    "    \"craftsmanship\",\n",
    "    \"durability\",\n",
    "    \"quality\",\n",
    "    \"reliable\",\n",
    "    \"precision\",\n",
    "    \"handcrafted\",\n",
    "    \"engineered\",\n",
    "    \"superior\",\n",
    "    \"authentic\",\n",
    "    \"resilient\",\n",
    "    \"premium\",\n",
    "    \"trusted\",\n",
    "    \"innovative\"\n",
    "]\n",
    "\n",
    "seed_words_quality = [\n",
    "    \"craftsmanship\", \"durability\", \"precision\", \"excellence\", \"superior\", \n",
    "    \"integrity\", \"workmanship\", \"innovation\", \"reliability\", \"tradition\", \n",
    "    \"expertise\", \"authenticity\", \"heritage\", \"quality\", \"trust\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = {\n",
    "    \"antiforeign\": seed_words_antiforeign,\n",
    "    \"fairness\": seed_words_fairness,\n",
    "    \"job_growth\": seed_words_job_growth,\n",
    "    \"military\": seed_words_military,\n",
    "    \"miu\": seed_words_miu,\n",
    "    \"pride\": seed_words_pride,\n",
    "    \"quality\": seed_words_quality\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7508235/7508235 [00:11<00:00, 650001.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams not added to index count:  29578\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "index = faiss.IndexFlatL2(384)\n",
    "\n",
    "corpus = open(\"corpus_replaced.txt\", \"r\").read()\n",
    "corpus = list(tokenize(corpus, lowercase=True))\n",
    "\n",
    "bigrams = []\n",
    "bigram_embeddings = []\n",
    "not_added = 0\n",
    "\n",
    "added_words_1 = {\n",
    "\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(corpus) - 1)):\n",
    "    if corpus[i] in model and corpus[i + 1] in model:\n",
    "        if not corpus[i] in added_words_1:\n",
    "            added_words_1[corpus[i]] = {}\n",
    "        if not corpus[i + 1] in added_words_1[corpus[i]]:\n",
    "            bigrams.append((corpus[i], corpus[i + 1]))\n",
    "            bigram_embeddings.append((model[corpus[i]] + model[corpus[i + 1]]) / 2)\n",
    "            added_words_1[corpus[i]][corpus[i + 1]] = True\n",
    "    else:\n",
    "        # print(\"Not added: \", corpus[i], corpus[i + 1])\n",
    "        not_added += 1\n",
    "\n",
    "index.add(np.array(bigram_embeddings))\n",
    "\n",
    "print(\"Bigrams not added to index count: \", not_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bigrams added to index:  1916114\n"
     ]
    }
   ],
   "source": [
    "print(\"Total bigrams added to index: \", len(bigram_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:51<00:00,  7.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "generated_words = \"\"\n",
    "generated_words_json = {}\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "filtered_seed_words = {}\n",
    "\n",
    "for category in tqdm(seed_words):\n",
    "    filtered_words = [word for word in seed_words[category] if word in model.key_to_index]\n",
    "    filtered_seed_words[category] = filtered_words\n",
    "    vectors = [model[word] for word in filtered_words]\n",
    "    average_vector = np.mean(vectors, axis=0)\n",
    "\n",
    "    similar_bigrams = index.search(average_vector.reshape(1, -1), 400)\n",
    "    similar_bigrams = [bigrams[i] for i in similar_bigrams[1][0]]\n",
    "    similar_bigrams = [f\"{bigram[0]} {bigram[1]}\" for bigram in similar_bigrams]\n",
    "\n",
    "    generated_words += f\"\\n\\n\\n## {category}:\\n\"\n",
    "    generated_words += \"\\n\".join(similar_bigrams)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Refine the keywords provided and provide a JSON array of the 100 best bigrams generated that are most refined/targeted towards the given category. Response JSON format: {\"refined_keywords\": [\"bigram1\", \"bigram2\", ...]}\"\"\"\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"# Category: {category}\\n\\n# Bigrams:\\n{json.dumps(similar_bigrams)}\"\n",
    "        }],\n",
    "        model=\"gpt-4o\",\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    response = json.loads(response)\n",
    "    refined_bigrams = response[\"refined_keywords\"]\n",
    "\n",
    "    removal = []\n",
    "    for word_1 in refined_bigrams:\n",
    "        for word_2 in refined_bigrams:\n",
    "            if word_1 in word_2 and word_1 != word_2:\n",
    "                removal.append(word_2)\n",
    "\n",
    "    refined_bigrams = [word for word in refined_bigrams if word not in removal]\n",
    "\n",
    "    generated_words += f\"\\n\\n\\n## {category} Refined Bigrams:\\n\"\n",
    "    generated_words += \"\\n\".join(refined_bigrams)\n",
    "\n",
    "    generated_words_json[category] = refined_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_words = \"# Seed words dictionary \\n\" + json.dumps(filtered_seed_words, indent=4) + generated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_words.txt\", \"w\") as f:\n",
    "    f.write(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_words.json\", \"w\") as f:\n",
    "    json.dump(generated_words_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 100 words close to this averaged vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len before:  102\n",
      "[\n",
      "    \"achieving\",\n",
      "    \"artistic\",\n",
      "    \"artistry\",\n",
      "    \"assurance\",\n",
      "    \"attention\",\n",
      "    \"authentic\",\n",
      "    \"capability\",\n",
      "    \"clarity\",\n",
      "    \"comfort\",\n",
      "    \"commitment\",\n",
      "    \"confidence\",\n",
      "    \"consistency\",\n",
      "    \"construction\",\n",
      "    \"craft\",\n",
      "    \"creating\",\n",
      "    \"creativity\",\n",
      "    \"culture\",\n",
      "    \"dedicated\",\n",
      "    \"dedication\",\n",
      "    \"dependability\",\n",
      "    \"dependable\",\n",
      "    \"detail\",\n",
      "    \"development\",\n",
      "    \"distinct\",\n",
      "    \"driven\",\n",
      "    \"durability\",\n",
      "    \"elegance\",\n",
      "    \"endurance\",\n",
      "    \"enduring\",\n",
      "    \"engineered\",\n",
      "    \"engineers\",\n",
      "    \"environment\",\n",
      "    \"excellence\",\n",
      "    \"exceptional\",\n",
      "    \"expectations\",\n",
      "    \"experienced\",\n",
      "    \"expertise\",\n",
      "    \"exquisite\",\n",
      "    \"extraordinary\",\n",
      "    \"fabrication\",\n",
      "    \"finest\",\n",
      "    \"flawless\",\n",
      "    \"greatest\",\n",
      "    \"groundbreaking\",\n",
      "    \"growth\",\n",
      "    \"heritage\",\n",
      "    \"highest\",\n",
      "    \"honesty\",\n",
      "    \"industry\",\n",
      "    \"innovation\",\n",
      "    \"integration\",\n",
      "    \"integrity\",\n",
      "    \"knowledge\",\n",
      "    \"leadership\",\n",
      "    \"legacy\",\n",
      "    \"longevity\",\n",
      "    \"masterful\",\n",
      "    \"materials\",\n",
      "    \"meticulous\",\n",
      "    \"perfection\",\n",
      "    \"performance\",\n",
      "    \"perseverance\",\n",
      "    \"philosophy\",\n",
      "    \"pioneering\",\n",
      "    \"precision\",\n",
      "    \"pride\",\n",
      "    \"professionalism\",\n",
      "    \"professionals\",\n",
      "    \"providing\",\n",
      "    \"pursuit\",\n",
      "    \"quality\",\n",
      "    \"refined\",\n",
      "    \"refinement\",\n",
      "    \"reliability\",\n",
      "    \"reliable\",\n",
      "    \"remarkable\",\n",
      "    \"resilience\",\n",
      "    \"rigorous\",\n",
      "    \"rugged\",\n",
      "    \"scientists\",\n",
      "    \"skill\",\n",
      "    \"sophistication\",\n",
      "    \"specialized\",\n",
      "    \"standards\",\n",
      "    \"strength\",\n",
      "    \"strong\",\n",
      "    \"superior\",\n",
      "    \"sustainable\",\n",
      "    \"technology\",\n",
      "    \"timeless\",\n",
      "    \"toughness\",\n",
      "    \"tradition\",\n",
      "    \"unmatched\",\n",
      "    \"unparalleled\",\n",
      "    \"unwavering\",\n",
      "    \"vision\",\n",
      "    \"workmanship\"\n",
      "]\n",
      "97 removed 5 words\n",
      "['authenticity', 'craftsmanship', 'distinctive', 'innovations', 'skilled']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "words = [\n",
    "    \"craftsmanship\", \"excellence\", \"longevity\", \"reliability\", \"integrity\", \n",
    "    \"innovation\", \"durability\", \"superior\", \"dedication\", \"unmatched\", \n",
    "    \"exceptional\", \"expertise\", \"precision\", \"quality\", \"refinement\", \n",
    "    \"standards\", \"authenticity\", \"workmanship\", \"dependability\", \"tradition\", \n",
    "    \"unparalleled\", \"strength\", \"commitment\", \"consistency\", \"remarkable\", \n",
    "    \"legacy\", \"elegance\", \"perfection\", \"knowledge\", \"toughness\", \n",
    "    \"professionalism\", \"skill\", \"performance\", \"capability\", \"dedicated\", \n",
    "    \"refined\", \"attention\", \"unwavering\", \"extraordinary\", \"industry\", \n",
    "    \"artistry\", \"enduring\", \"resilience\", \"highest\", \"specialized\", \n",
    "    \"innovations\", \"providing\", \"engineers\", \"pursuit\", \"detail\", \n",
    "    \"technology\", \"masterful\", \"timeless\", \"professionals\", \"finest\", \n",
    "    \"skilled\", \"artistic\", \"authentic\", \"heritage\", \"materials\", \n",
    "    \"rigorous\", \"honesty\", \"vision\", \"assurance\", \"greatest\", \n",
    "    \"strong\", \"engineered\", \"creating\", \"sustainable\", \"growth\", \n",
    "    \"fabrication\", \"scientists\", \"driven\", \"craft\", \"culture\", \n",
    "    \"flawless\", \"exquisite\", \"dependable\", \"pride\", \"creativity\", \n",
    "    \"meticulous\", \"achieving\", \"groundbreaking\", \"development\", \n",
    "    \"reliable\", \"expectations\", \"experienced\", \"perseverance\", \n",
    "    \"environment\", \"construction\", \"sophistication\", \"pioneering\", \n",
    "    \"rugged\", \"comfort\", \"philosophy\", \"confidence\", \"clarity\", \n",
    "    \"endurance\", \"distinctive\", \"leadership\", \"distinct\", \"integration\"\n",
    "]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "removal = []\n",
    "for word_1 in words:\n",
    "    for word_2 in words:\n",
    "        if word_1 in word_2 and word_1 != word_2:\n",
    "            removal.append(word_2)\n",
    "\n",
    "print(\"Len before: \", len(words))\n",
    "\n",
    "words = [word for word in words if word not in removal]\n",
    "print(json.dumps(words, indent=4))\n",
    "print(len(words), f\"removed {len(removal)} words\")\n",
    "print(removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
